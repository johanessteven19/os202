---
permalink: /W07/
---
[Back to Home](../)

<br>
# Top 10 List of Week 07

1. [Race Conditions.](https://searchstorage.techtarget.com/definition/race-condition#:~:text=A%20race%20condition%20is%20an,sequence%20to%20be%20done%20correctly.)<br>
Last week we have learned about concurrency, in which multiple sequences of instructions are executed simulataneously. While this is very efficient and useful in the context of Operating Systems, there are problems that may appear from parallel execution. The **Race Condition** is one such issue, in which multiple processes are running in parallel and they need access to a certain data that is shared between the multiple processes. The final result will then depend on which process is completed first. Since the processes are executed conccurently, the process that will be completed first is random every execution cycle, therefore resulting in the different final result everytime. This is obviously an undesirable situation that has to be resolved.

2. [Critical Section ](https://medium.com/@yash.kukreja.98/day-12-cs-fundamentals-december-about-operating-systems-critical-section-problem-c70e457c619a)<br>
**Critical Sections** are certain parts of your written code that involves accessing shared data that may be manipulated. Of course, this may result in the previously mentioned Race Conditions (which we do not want). Therefore, we have to make sure that these **Critical Sections** are properly equipped to prevent undesirable conditions that may result in corrupted values of your shared data. To do that, the critical sections have to fulfill three criterias:
* **Mutual Exclusion**, which means that only a single process is allowed to execute the critical section at any given time.
* **Progress**, when the critical sections is free, meaning it is not being executed by any process, the next process to enter the critical section is cooperatively determined.
* **Bounded Waiting**, any process still waiting to enter the critical section is guaranteed to enter before its waiting time limit is up.

3. [Critical Section Solutions.](https://medium.com/@yash.kukreja.98/day-12-cs-fundamentals-december-about-operating-systems-critical-section-problem-c70e457c619a)<br>
There are differing solutions to prevent Race Conditions from occuring, for example:
* **TestAndSet** (Hardware Solution), in which a 'shared lock' is created. This shared lock may only have two different values, which are either **0** or **1**. Each time a process would like to enter the critical section, a separate process will check the value of the lock. When a process enters the critical section, the lock value becomes **1**. When the lock is activated (value is **1**), no other process may enter the critical section. When the process exits the critical section, the lock becomes free (value is **0**), and some other process may enter the critical section.
* **Semaphores** (int variable), such as:
    - **Binary Semaphores**, in which the semaphore may only have two different values, either **0** or **1**. The semaphore is shared between all processes and works in the same way as 'shared lock', therefore it can also be called the *mutex lock*.
    - **Counting Semaphores**, in which the semaphore can be any whole numbers. The values are used to limit access to a shared resource. When a resource can only allow 5 processes to simultaneously access it, the semaphore will first start at **5**. Each time a process accesses it, the semaphore is decremented by **1** until it reaches **0**. When the semaphore is **0**, then the resource is locked until a process exits from it, incrementing the value to **1**.
* **Peterson's Solution** (Software Solution), will contain two shared variable, a boolean array flag (size 2), and an integer to show which process may enter the critical section. The basic idea of this solution is to modify the flag and integer values each time a process enters and exits the critical section. When a process enters, the flag is set to **True**, and the integer takes the index of the second process. Therefore, the second process will execute first before the first process which is still waiting. When the second process is finished, the first process will then enter the critical section. When it is done executing, the flag is set back to **False**. 

4. [Deadlocks.](https://www.tutorialspoint.com/process-deadlocks-in-operating-system)<br>
Deadlocks are events in which the OS is stuck in a permanent waiting state. For example, imagine there are two processes running at the same time. Process 1 is currently running while accessing Resource 1. Process 2 is currently running while accessing Resource 2. To complete it's instruction sequence, Process 1 needs to access Resource 2, which is currently occupied by Process 2. On the other hand, Process 2 needs to access Resource 1, which is currently occupied by Process 1. Since no process can continue due to the resources being occupied, the OS is in a deadlock state. 

5. [Process Control Block.](https://www.tutorialspoint.com/operating_system/os_processes.htm)<br>
The Process Control Block (PCB) is the data structure maintained by the OS that represents every single process in the OS. A PCB will store informations about each process to keep track of it. The informations stored are:
* Process State, either **Ready**, **Running**, **Waiting**, etc.
* Process Privileges, to keep track of which resources are available to the process.
* Process ID, to identify each process in the OS.
* Pointer, the pointer to the parent process.
* Program Counter, the pointer to the address of the next instruction to be executed.
* CPU Registers, running processes are stored in these registers to be executed.
* CPU Scheduling Information, shows which process has priority to be executed first and information about process execution schedules.
* Memory Management Information, informations about the page table, memory limits, etc.
* Accounting Information, amount of CPU needed for the execution, time limits, execution IDs, etc.
* I/O Status Information, information regarding I/O devices accessible by the process.

6. [Process scheduling.](https://www.tutorialspoint.com/operating_system/os_process_scheduling.htm)<br>
Process scheduling happens when running process is removed from the CPU and another process is selected by the process manager to be executed next. The selection is done based on a pre-determined strategy. This allows for the efficient loading and parallel execution of processes into the executable memory. There are three important process scheduling queues that is maintained by the OS:
* **Job Queue**, keeps all processes in the system.
* **Ready Queue**, keeps a set of all processes stored in the main memory to be ready and waiting to be executed. Every new process will be placed in this queue.
* **Device Queue**, filled with every process that is blocked by the unavailability of a specific I/O device to be executed.

7. [Threads.](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/4_Threads.html#:~:text=A%20thread%20is%20a%20basic,out%20at%20any%20given%20time.)<br>
Threads are basic units of CPU utilization, each one consisting of a Program Counter, a stack, a set of registers, and a thread ID. In single-threaded applications, processes will be controlled by a single thread, meaning that there will only be one Program Counter, and only one set of instructions to be executed at any given moment. The opposite of that is the multi-threaded process, in which there could exist multiple threads in a single process, with their own Program Counters, stacks, and sets of registers. They however will still share a common code, data, and files.

8. [Process vs. Thread.](https://www.tutorialspoint.com/operating_system/os_multi_threading.htm)<br>
* A process will need a lot of resources to be executed, while a thread is very light weight.
* When multiple processes are executed at the same time, each one will execute the same code, but will take up their own memory and file resources. In multiple threads on the other hand, they will share the same set of open files and child processes.
* To switch processes, the OS has to be notified. To switch threads, we don't need to interact with the OS, thus reducing time and resource needed.
* When a previous process is blocked, then the next processes will not be able to be executed until it is unblocked again. This is not true for threads, which can run a second thread when the first one is blocked.
* A process will run independently of another process, while a thread can read or write the other thread's data. 

9. [Implementing threads.](https://www.tutorialspoint.com/operating_system/os_multi_threading.htm)<br>
There are two main ways of implementing threads, which are:
* **User Level Threads**, which are user-managed. The thread management kernel will not be aware of the running threads, therefore the thread library itself will contain code for the creation and destruction of threads, thread scheduling, and saving/restoring threads. This type of threads is much simpler, not requiring Kernel mode privileges, able to be run on any OS, and faster to be created and managed. They however, will not be able to take advantage of multiprocessing.
* **Kernel Level Threads**, which are OS-managed threads acting on the kernel. The kernel will be responsible for managing the existing threads. This allows the kernel to be able to schedule multiple threads from the same process on multiple processes at the same time, therefore granting much higher efficiency. This type of threads however, will be slower to create and manage than the User Level Threads. Also, when switching control from one thread to another, they will have to access the Kernel first, before being able to switch to the other thread.

10. [Multithreading models.](https://www.tutorialspoint.com/operating_system/os_multi_threading.htm)<br>
There are three main models of multithreading, each with their own uses:
* **Many-to-Many Model**. This model will multiplex any number of user level threads into any number of kernel level threads that are equal or less to the number of user level threads. This is useful because the developer is able to create as many threads as they want and they will be able to run parallel to the kernel level threads on a multi-processor machine. This model will provide the best accuracy on conccurency compared to the other models.
* **Many-to-One Model**. This model will map any number of user level threads into a single kernel level thread. These threads will be managed by the thread library (e.g. Windows, Pthreads, Java threading). This model will not allow for parallel execution, as only a single thread at a time can access the kernel. This model will only be implemented if the user level thread libraries are not supported by the OS.
* **One-to-One Model**. This model will map a single user level thread into its corresponding kernel level thread. This allows for more conccurency to occur compared to the previous model. This model will also let a separate thread to run when the first thread generate a blocking system call. However, this also means that the more user level threads there are, the more kernel level threads are created.
